diff --git a/nuScenes/sinbevt/__init__.py b/nuScenes/sinbevt/__init__.py
deleted file mode 100644
index e69de29..0000000
diff --git a/nuScenes/sinbevt/callbacks/gitdiff_callback.py b/nuScenes/sinbevt/callbacks/gitdiff_callback.py
deleted file mode 100644
index 9287f3e..0000000
--- a/nuScenes/sinbevt/callbacks/gitdiff_callback.py
+++ /dev/null
@@ -1,37 +0,0 @@
-import logging
-import pytorch_lightning as pl
-import git
-
-from pathlib import Path
-from pytorch_lightning.utilities import rank_zero_only
-from omegaconf import OmegaConf, DictConfig
-
-
-log = logging.getLogger(__name__)
-
-
-PROJECT_ROOT = Path(__file__).parent.parent.parent
-TEMPLATE = """
-==================================================
-{diff}
-==================================================
-{cfg}
-==================================================
-"""
-
-
-class GitDiffCallback(pl.Callback):
-    """
-    Prints git diff and config
-    """
-    def __init__(self, cfg: DictConfig):
-        super().__init__()
-
-        self.cfg = cfg
-
-    @rank_zero_only
-    def on_fit_start(self, trainer: pl.Trainer, pl_module: pl.LightningModule) -> None:
-        diff = git.Repo(PROJECT_ROOT).git.diff()
-        cfg = OmegaConf.to_yaml(self.cfg)
-
-        log.info(TEMPLATE.format(diff=diff, cfg=cfg))
diff --git a/nuScenes/sinbevt/callbacks/visualization_callback.py b/nuScenes/sinbevt/callbacks/visualization_callback.py
deleted file mode 100644
index 2dd224b..0000000
--- a/nuScenes/sinbevt/callbacks/visualization_callback.py
+++ /dev/null
@@ -1,56 +0,0 @@
-import pytorch_lightning as pl
-import torch
-import torch.utils.data
-
-from typing import Any, Optional
-from pytorch_lightning.utilities.types import STEP_OUTPUT
-from pytorch_lightning.loggers.wandb import WandbLogger
-from pytorch_lightning.utilities import rank_zero_only
-from pytorch_lightning.utilities.warnings import rank_zero_warn
-
-
-class VisualizationCallback(pl.Callback):
-    def __init__(self, visualizers, log_image_interval=1000):
-        super().__init__()
-
-        self.visualizers = {'image': visualizers}
-        self.log_image_interval = log_image_interval
-
-    def on_train_batch_end(
-        self,
-        trainer: pl.Trainer,
-        pl_module: pl.LightningModule,
-        outputs: STEP_OUTPUT,
-        batch: Any,
-        batch_idx: int,
-        *args,
-        **kwargs
-    ) -> None:
-        if batch_idx % self.log_image_interval == 0:
-            self._visualize_batch(outputs, trainer, batch_idx, 'train')
-
-    def on_validation_batch_end(
-        self,
-        trainer: pl.Trainer,
-        pl_module: pl.LightningModule,
-        outputs: Optional[STEP_OUTPUT],
-        batch: Any,
-        batch_idx: int,
-        *args,
-        **kwargs
-    ) -> None:
-        if batch_idx % self.log_image_interval == 0:
-            self._visualize_batch(outputs, trainer, batch_idx, 'val')
-
-    @rank_zero_only
-    def _visualize_batch(self, outputs: STEP_OUTPUT, trainer: pl.Trainer, batch_idx: int, prefix: str):
-        for key, viz in self.visualizers.items():
-            self._log_image(viz(**outputs), f'{prefix}/{key}', trainer.logger)
-
-    def _log_image(self, image_batch, tag, logger):
-        if isinstance(logger, torch.utils.tensorboard.writer.SummaryWriter):
-            logger.add_images(tag=tag, img_tensor=torch.from_numpy(image_batch), dataformats='NHWC')
-        elif isinstance(logger, WandbLogger):
-            logger.log_image(tag, image_batch)
-        else:
-            rank_zero_warn(f'Invalid logger {logger}')
